## ai 시작
- 인공지능은 로봇이다 ? no - 그냥 software
- 인공지능은 스스로 똑똑해질수 있다 ? no , 90% 이상이 지도학습중
- 인공지능도 감정이 있다 ? no , 감정이 있는것처럼 프로그래밍 하는것
- 강인공지능 : 다양한 상황에 유연하게 대처하며 사람과 자연스럽게 대화,
스스로 판단하고 결정내림, 범 인공지능이라고 하기도함
- 약인공지능 : 특정분야 / 업무를 수행하는 인공지능
- AI > 머신러닝 > 딥러닝
  - AI : 모든 자동화에 해당
  - 머신러닝 : 특징추출은 사람이, 판단은 머신 (회귀분석, 의사결정나무)
  - 딥러닝 : 특징추출 / 판단 모두 머신(CNN,RNN)

## 머신러닝 / 딥러닝 / 인공신경망
- 머신러닝
  - 테이블(엑셀)형테의 정형데이터 처리 (R, SAS, SPSS 같은 통계분석툴 활용)
  - 선형/로지스틱 회귀분석, 의사결정나무, ARMA,ARIMA 모형이 대표적임
- 딥러닝
  - 기사, SNS, 이미지 등 비정형데이터 처리
- 인공신경망
  - 인공뉴런
    - 생물학적 뉴런의 모양을 본따서 만듦
    - 이전 뉴런이 넘겨준 데이터를 가중함 연산 + 비선형 함수를 적용하여 다음 뉴런은로전달
    - 인공뉴런을 여러층 쌓으면 인공 신경망
## 이미지처리
  - 3차원(TENSOR) : 2차원배열 3개 (각각 R,G,B)
  - CNN 
    - 특징추출(Feature Extraction) 과, task 수행영역으로 나누어 처리
    - 특징추출
      - 컨볼루션 연산(특징찾기, 가중합+비선형)과 풀링연산(핵심특징 추출) 수행
    - task
      - classfication : 종 분류 (ex : 개, 소, 고양이)
      - detection : 이미지내 좌표 추적 (ex : 카메라 사람얼굴 인식 박스)
      - segmetation : 필셀단위 추적 (ex : 자율주행 사물인식, 경계를 필셀단위로 파악)

## 텍스트 처리
- 자연어 이해(NLU)
    - 자연어 : 한글, 영어, 중국어 등
    - 인공어 : 프로그래밍 언어, 에스페란토
    - 일반적으로 NLP(자연어처리) 는 NLU(자연어이해) 를 포함하나 요즘들어 경계는 불명확
    - NLP - 형태소분석, 개체명 인식, 의존구문분석 + NLU
    - NLU - 감성, 의도, 의미 질의 응답 등
- 기계에 언어인식
    - 1.Tokenizint(Parsing) : 어절, 형태소, 음절, 자소 (한글은 특성상 형태소, 음절 주로 사용)
      - 어절 : 한글에 부적합
      - 형태소 : 정제된글에 강함, 맞춤법이 틀리거나 신조에 취약,형태소 분석기에따라 결과달라짐
      - 음절 : vocabulary list 13000개 수준, 오타에 덜민감
      - 자소 : vocabulary list 몇백개 수준
    - 2.워드 임베딩 : 인공신경망이 계산 가능하도록 vector 화
      - 원-핫 인코딩 : 모든토큰을 중복제거하여 사전을 만든다
        - 인코딩은 간단하나, 굉장히 길이간 긴 벡터가 필요
      - CBOW 
        - 단어를 특정 길이를 가진 임의 벡터로 생성 (원핫인코딩보다 훨씩 작음)
        - 인공지능에게 문장을 주고 빈칸을 유추하도록 러닝 
      - SKIPGRAM
        - 단어를 특정 길이를 가진 임의 벡터로 생성 (원핫인코딩보다 훨씩 작음)
        - 인공지능에게 단어를 주고 문맥을 만들도록 러닝
- 대표 자연어 이해 타스크
  - 문장 / 문서 분류(sentence/document classfication)
    - 입력받은 텍스트를 클래스(카테고리) 분류하는 과제 (ex : 감정분석(긍/부정), 챗봇 기능 맵핑)
  - Sequence to Sequence
    - 문장 / 문서를 입력받아 문장을 출력 (ex : 긴문서 요약 , 번역)
  - 질의응답(Question Answering)
    - 질문을 받아 매뉴얼중 가장 가능성이 높은 영역을 리턴하는 MRC(Machine Reading comprehension)
    - 유사한 과거 질문(FAQ)를 뽑는 IR (Information Retrieval)
    - 주로 챗봇 / 콜센터 활용
# 과거 데이터 분석을 통한 처리
- 시계열 데이터 처리
  - 주식 / 기상과 같이 과거의 데이터를 기반으로 예측 
- 순환신경망 ( RNN :  Recurrent Neural Network )
  - 장점
    - 시간 흐름에 다른 과거정보를 누적할 수 있다.
    - 가변길이의 데이터를 처리할 수 있다.
    - 다양한 구성의 모델을 만들 수 있다.
  - 단점
    - 연산속도가 느리다. (과거데이터부터 순차로 처리하기 때문)
    - 학습이 불안정하다
      - timestep 가 너무길어지면, 학습량 폭발(gradient exploding)
      - timestemp 가 길어지면 먼 과거의 데이터가 잊혀진다 (Gradient Vanishing)
    - 장기 종속성/의존성 문제(long term dependency)
- RNN 성능보와
  - LSTM(Long Short term memory)
    - 먼과거의 중요한 자료는 기억하고, 불필요한것은 버리는 RNN
    - forget gate, input gate, ouput gate 로 구성됨
    - RNN 보다 예측은 잘하나 , 속도는 더 느려짐
    - 비슷한것으로 GRU 가 있음
    - ex) 태양광 에너지 발전량 에측, 번역도 단어 순서대로 시계열처리가능(sequence to sequence)
## 좋은 인공지능 만들기
- 오프라인 프로세스(offline process)
  - 오픈전 모델 최적화에 해당
  - Training Pipeline
    - 기존 데이터를 정제하여 필요하느 부분을 취하거나 라벨을 붙인다.
    - 데이터가 마련되면 좋은 성능을 달성할때까지(validate & select model) 반복실험 진행 -> 튜닝
    - 배포를 위해 최종모델 선택(publish model)
- 온라인 프로세스(online process)
  - 오픈후 운영단계에 해당 -> 개발에 가까움 ai 모델을 운영환경에 띄우고, 화면 개발 , 고객사 db 연결 등
  - 이제부터는 운영환경 스트리밍 데이터(live data) 처리
- 오버피팅(overfitting)과 일반화(generalization)
  - 일반화(generalization) : 이전에 본적 없는 데이터에 대해서도 잘 수행하는 능력
  - 일반화 능력이 떨어지며 오버피팅된 상태임(새로운 데이터 처리 불가, 기존 답만 제대로 수행)
  - 오버피팅을 회피하고 일반화를 잘하기 위해 , 기존을 데이터를 Training, Validation, Test 로 나누어 사용한다.(8:2:2,9:1:1)
    - Traning set : 학습용 데이터, 정답이 있는 데이터
    - Validation set : 정답을 알려주지 않음, 이 데이터를 통해 모델 튜닝
    - Test set : 최종 성능 평가용 데이터
- 학습곡선(learning curve) 확인하기
  - Traning set 의 정답률이 올라가나, validation set 의 정답률은 올라가지 않기 시작하면 오버피팅
- Regularization : 일반화(generaliztion) 성능향상이 목적임
  - 데이터 증강 : 데이터를 변조하여 더많이 확보하는 방식
    - 이미지 반전, 크롭, 노이즈, 생상, 명암 , 채도 변화 등과같은 방식
  - Capacity 줄이기
    - Capacity 는 모델의 복잡한 정도를 나타냄
    - 신경망이 여러층이거나, 뉴런수가 많아질면 높아짐
    - Capacity 가 필요이상으로 높으면 그냥 데이터 외움
  - 조기종료(Early Stopping)
    - 오버피팅이 감지되면 조기종료(validation set 의 개선이 없을때)
  - 드롭아웃(Dropout)
    - 일정비율의 노드(인공뉴런)을 무작위로 끄고 진행.

## 인공지능 재활용
- 인공지능 개발의 어려움
  - 구체적이지 않으며 불평확한 타스크
  - 적은 데이터, 낮은 품질의 데이터
  - 다른 도메인 환경(적용환경의 차이에 따른 문제(영상처리시, 조도 각도 등))
- Transfer Learning ( 전이학습 )
  - 기존에 만들어진 딥러닝 모델을 재활용하여 사용하는 기법
  - 이미 만들어 놓은 모델의 아키텍처를 새태스크에 맞게 조금 수정
  - Catastrophic Forgetting (치명적 기억상실 ) : transfer 가 많이되면 기존 데이터를 잊게됨
  - Transfer 를 잘하려면?
    - 레이어 동결 - 기존데이터를 처리하는 전반부 는 동결, 새타스크를위한 후반부 진행
    - Discriminative fine tuning - 전반부는 조금만 공부, 후반부는 많이 공부

## 준비된 인공지능
- Pre-Training (사전학습) : Transfer Learning 을 염두에 두고 여러 지식을 미리 학습
  - 이미지 : 이미지넷
  - 영상 : Youtube-8m, 
  - 언어 : 위키피디아, 나무위키, 세종말뭉치 학습
- Self Supervised Learning( 자가지도 학습)
  - 대규모 데이터는 라벨이 잘 없음, 이런경우 사용
  - 기계가 라벨을 만들어서 사용

## 족집게 학습
- Active Learning(능동학습) : 데이터는 있으나, 라벨이 없고, 라벨을 만들 인력이 있으나, 부족한 경우 사용
- ex : 의료 이미지 인식 -> 의사를 계속 라벨링하라고 할수는 없으니...
- 인공지능 모델이 맞추기 어려운 데이터를 골라서 집중학습
- Active Learning 절차
  - Training a Model : 초기학습 데이터(라벨링된)를 학습
  - Select query : 라벨되지 않은 데이터 중, 어려운 데이터 선별
  - human labeling : 선별된 데이터 라벨링
  - 선별한 라벨 데이터를 기존학습 데이터와 병합한 후 다시 모델 학습
- Query Strategy : 어떻게 잘모르는 데이터를 선별할지?
  - Uncertainty Sampling : 학습된 모델의 판정값을 기반으로 뽑는다. -> ai 가 불확실하다고 판단하는 데이터 추출
  - Query By Commitee : 여러 ai 모델이 자주틀리는 데이터 뽑는다.
  - Expected Impact : 데이터가 추가될때, 학습된 모델이 가장 많이 변화하는데이터 선별
  - Density Weighted mothod : 밀집된 데이터 선별
  - Core-set approach : 데이터를 최대한 고르게 선별하여 전체 분표를 대표할수 있게함
## 중요데이터 추출
많은 데이터중 중요데이터를 선별할수 있어야한다.
- Attention mechanism(어텐션 메커니즘)
  - RNN 의 경우 오래된 데이터는 망각한다. 하지만 어텐션 메커니즘을 통해 과거데이터중 중요부분에 집중하게함
  - 어텐션 스코어(Attention score)
    - 어텐션 스코어는 인공신경망 모델이 각 인코딩 timestep마다 계산된 특징(feature)를 가지고 자동으로 계산하는 0~1사이의 값입니다.
  - 컨텍스트 벡터(Context Vector)
    - 이렇게 어디를 더 살펴보고 어디는 대충 볼지에 대해 어텐션 스코어를 구하고 나면 현재 디코딩할 단어와의 관련성을 반영하여 다시 입력 문장을 인코딩하게 되는데 이는 중요도에 따라 전체 문맥의 정보를 잘 반영하고 있다고 하여 컨텍스트 벡터(Context vector)라고 부릅니다.
- XAI 로서의 어텐션
  - 어텐션 메커니즘은 기계가 판단시 중요하게 생각하는 부분을 우리에게 알려준다. 이를 통해 우리는 결과를 해석할 수 있다. 이를 해석가능한 인공지능(interpretable ai)라고한다.

## 스스로 진화하는 인공지능
- AutoML : 자동화된 기계학습
- AutoML 의 역할
  - Feature Engineering 자동화 : AI 모델을 학습하기 위해 데이터로부터 중요한 특징을 선택하고 인코딩하는 방식 자동화
  - 하이퍼 파라미터 자동탐색 : AI 모델 학습에 필요한 사람의 설정등???
    - 그리드 서치 : 최적화할 하이퍼 파라미터의 값 구간을 일정 단위로 나눈 후, 각 단위 조합을 테스트하여 가장 높은 성능의 조합 선택
      - 단순하지만 하이퍼 파라미터 가 많다면 경우의수가 너무 많아져 오래걸림
    - 랜덤 서치 : 랜던한 조합을 테스트, 그리드에 비교적 빠르게 최적조합 찾음
    - Meta Learner : 최근에는 위의 두가지 방식보다는, 하이퍼파라미터도 모델을 통해 탐색한다. 메타 러너는 RNN 과 강화학습을 통해 하이퍼 파라미터 탐색
  - 아키텍처탐색 : AI 모델의 구조 자체를 더 효율적인 방향으로
    - NAS(Neural Architecture search) : 인공신경망용 아키텍처 탐색 방법
      -  Meta Learner와 Learner로 이루어져 있어서, Learner가 본 과제를 수행하는 AI 모델이라면 Meta Learner가 어떤 구조의 신경망을 만들면 좋은지, 아키텍쳐 구성을 고민하게 됩니다. Meta Learner는 역시 RNN과 강화학습을 접목한 형식으로 구성해볼 수 있습니다. Meta Learner는 Learner의 인공신경망 아키텍처가 어떻게 구성되면 좋을지를 결정하여, Learner의 태스크 수행 결과를 보상으로 활용합니다.
  - CSP 업체들도 AutoML 제공

## 설명가능한 인공지능 (XAI)
딥러닝의 경우 특징 추출부터 판단까지 AI 가 수행하므로 AI 가 판단한 결과가 어떻게 만들어졌는지 추적이 어려움
인공신경망으로 만들어진 AI 는 복잡한 구조로인해 블랙박스로 불려짐

- XAI 수행방법 3가지
  - 기존 AI 모델에 설명할수 있는 모듈을 붙이는방식
    - 어텐션 메커니즘을 활용한 XAI : 딥러닝 모델이 어떤 결정을 내릴때, 입력데이터의 어떤부분에 집중해서 판단했는지를 시각화 
    - 설명방법 학습 ( Learn to explain ) : 딥러닝 모델에 RNN 모듈등을 덧붙여 인간이 이해할수있는 설명을 생성하도록 진행
    - 모듈러 네트워크 : 판정결과가 어떤 모듈경로를 따라 연산되는지 파악
    - Feature Identification : 설명가능한 특징을 학습한 노드를 찾아 그 특징에 설명라벨 붙임
  - 애초에 설명력이 있는 모델을 만드는 방법
    - 의사경정나무, 설형회귀분석은 설명럭이 있는 모델임, 딥러닝은 설명력이 부족한 인공신경망을 사용하므로 딥러닝에서는 미사용
  - 인공신경망처럼 복잡한 블랙박스 모델의 일부분을 설명해 줄 수 있는 다른 모델을 활용하여 유추
    - ex) LIME , SP-LIME
