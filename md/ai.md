## 1.ai 시작
- 인공지능은 로봇이다 ? no - 그냥 software
- 인공지능은 스스로 똑똑해질수 있다 ? no , 90% 이상이 지도학습중
- 인공지능도 감정이 있다 ? no , 감정이 있는것처럼 프로그래밍 하는것
- 강인공지능 : 다양한 상황에 유연하게 대처하며 사람과 자연스럽게 대화,
스스로 판단하고 결정내림, 범 인공지능이라고 하기도함
- 약인공지능 : 특정분야 / 업무를 수행하는 인공지능
- AI > 머신러닝 > 딥러닝
  - AI : 모든 자동화에 해당
  - 머신러닝 : 특징추출은 사람이, 판단은 머신 (회귀분석, 의사결정나무)
  - 딥러닝 : 특징추출 / 판단 모두 머신(CNN,RNN)

## 2.머신러닝 / 딥러닝 / 인공신경망
- 머신러닝
  - 테이블(엑셀)형테의 정형데이터 처리 (R, SAS, SPSS 같은 통계분석툴 활용)
  - 선형/로지스틱 회귀분석, 의사결정나무, ARMA,ARIMA 모형이 대표적임
- 딥러닝
  - 기사, SNS, 이미지 등 비정형데이터 처리
- 인공신경망
  - 인공뉴런
    - 생물학적 뉴런의 모양을 본따서 만듦
    - 이전 뉴런이 넘겨준 데이터를 가중함 연산 + 비선형 함수를 적용하여 다음 뉴런은로전달
    - 인공뉴런을 여러층 쌓으면 인공 신경망
## 3.이미지처리
  - 3차원(TENSOR) : 2차원배열 3개 (각각 R,G,B)
  - Convolutional Neural Network(CNN) 
    - 특징추출(Feature Extraction) 과, task 수행영역으로 나누어 처리
    - 특징추출
      - 컨볼루션 연산(특징찾기, 가중합+비선형)과 풀링연산(핵심특징 추출) 수행
        - 컨볼루션 연산 : 컨볼루션 필터(또는 커널)가 입력 이미지를 상하좌우로 훑으며 주요한 특징이 있는지 찾아내는 과정
        - Feature map(또는 convolved feature) : 컨볼루션 연산의 결과물
        - 풀링연산 : Feature map을 역시 상하좌우로 훑으며 핵심적인 정보만을 영역별로 샘플링 하는데요, 주로 영역 내 가장 큰 값만을 남기고 나머지 값을 버리는 MaxPooling 방식을 적용
    - task
      - classfication : 종 분류 (ex : 개, 소, 고양이)
      - detection : 이미지내 좌표 추적 (ex : 카메라 사람얼굴 인식 박스)
      - Segmentation : 필셀단위 추적 (ex : 자율주행 사물인식, 경계를 필셀단위로 파악)

## 4.텍스트 처리
- 자연어 이해(NLU)
    - 자연어 : 한글, 영어, 중국어 등
    - 인공어 : 프로그래밍 언어, 에스페란토
    - 일반적으로 NLP(자연어처리) 는 NLU(자연어이해) 를 포함하나 요즘들어 경계는 불명확
    - NLP - 형태소분석, 개체명 인식, 의존구문분석 + NLU
    - NLU - 감성, 의도, 의미 질의 응답 등
- 기계에 언어인식
    - 1.Tokenizint(Parsing) : 문장을 특정 세부단위로 쪼개는것(어절, 형태소, 음절, 자소) (한글은 특성상 형태소, 음절 주로 사용)
      - 어절 : 한글에 부적합
      - 형태소 : 정제된글에 강함, 맞춤법이 틀리거나 신조에 취약,형태소 분석기에따라 결과달라짐
      - 음절 : vocabulary list 13000개 수준, 오타에 덜민감
      - 자소 : vocabulary list 몇백개 수준
    - 2.워드 임베딩 : 쪼개진 토큰을 벡터로 바꾸는 과정
      - 원-핫 인코딩 : 모든토큰을 중복제거하여 사전을 만든다
        - 인코딩은 간단하나, 굉장히 길이간 긴 벡터가 필요
      - CBOW 
        - 단어를 특정 길이를 가진 임의 벡터로 생성 (원핫인코딩보다 훨씩 작음)
        - 인공지능에게 문장을 주고 빈칸을 유추하도록 러닝
        - 단어 간의 유사성에 따라 벡터 유사성이 나타난다.
      - SKIPGRAM
        - 단어를 특정 길이를 가진 임의 벡터로 생성 (원핫인코딩보다 훨씩 작음)
        - 인공지능에게 단어를 주고 문맥을 만들도록 러닝
- 대표 자연어 이해 타스크
  - 문장 / 문서 분류(sentence/document classfication)
    - 입력받은 텍스트를 클래스(카테고리) 분류하는 과제 (ex : 감정분석(긍/부정), 챗봇 기능 맵핑)
  - Sequence to Sequence
    - 문장 / 문서를 입력받아 문장을 출력 (ex : 긴문서 요약 , 번역)
  - 질의응답(Question Answering)
    - 질문을 받아 매뉴얼중 가장 가능성이 높은 영역을 리턴하는 MRC(Machine Reading comprehension)
    - 유사한 과거 질문(FAQ)를 뽑는 IR (Information Retrieval)
    - 주로 챗봇 / 콜센터 활용
## 5.과거 데이터 분석을 통한 처리
- 시계열 데이터 처리
  - 주식 / 기상과 같이 과거의 데이터를 기반으로 예측 
- 순환신경망 ( RNN :  Recurrent Neural Network )
  - 과거에 데이터를 처리하여 결과를 출력했던 과정의 일부를 가져와 현 시점에서 데이터를 처리하고 결과를 출력하는 데 도움을 주는 방식, 이 때 입력 데이터의 정보를 누적하는 부분을 인코딩(Encoding), 결과를 출력하는 부분을 디코딩(Decoding)이라고 표현
  - 장점
    - 시간 흐름에 다른 과거정보를 누적할 수 있다. -> 과거 timestep 의 처리내역을 반영하여 결과 도출
    - 가변길이의 데이터를 처리할 수 있다. -> 일, 월, 시간, 초와 같이 원하는 구성으로 데이터 처리 가능
    - 다양한 구성의 모델을 만들 수 있다. -> 하나의 input 에서 여러 output, 여러 input 에서 하나의 output 과 같이 다양한 구성 가능
  - 단점
    - 연산속도가 느리다. (과거데이터부터 순차로 처리하기 때문 - GPU 장점인 병렬연산을 잘 활용하기 어려움) 정형데이터(수치,범주 등)에서는 속도저하를 체감하기 힘드나, 텍스트 데이터에 주로 문제가 됨
    - 학습이 불안정하다
      - rnn 은 학습이 매우 어려운 알고리즘중 하나임
      - timestep 가 너무길어지면, 학습량 폭발(gradient exploding)
      - timestep 가 길어지면 먼 과거의 데이터는 현재 추론에 영향을 거의 미치지 못함 (Gradient Vanishing)
    - 실질적으로 과거정보를 잘 활용하기 어려움 - 장기 종속성/의존성 문제(long term dependency) - 먼 과거의 데이터는 현재 추론에 영향을 거의 미치지 못함 (Gradient Vanishing 과 long term dependency 차이 모르겠음)
- RNN 성능보완
  - LSTM(Long Short term memory)
    - 먼과거의 중요한 자료는 기억하고, 불필요한것은 버리는 RNN
    - forget gate, input gate, ouput gate 로 구성됨
    > - forget gate : 과거의 정보 중 불필요한 부분은 통과시키지 않고 잊어버림
    > - input gate : 현재 input date 를 반영할지, 별로 중요하지 않으니 거를지 판단
    > - output gate : 연산된 최종 정보를 다음 시점으로 얼마나 전달할지 결정.
    - RNN 보다 예측은 잘하나 , 속도는 더 느려짐
    - 비슷한것으로 GRU 가 있음
    - ex) 태양광 에너지 발전량 에측, 번역도 단어 순서대로 시계열처리가능(sequence to sequence)
## 6.좋은 인공지능 만들기
- 오프라인 프로세스(offline process)
  - 오픈전 모델 최적화에 해당
  - Training Pipeline
    - **Generate features** & **Collect lables** : 기존 데이터를 정제하여 필요하느 부분을 취하거나 라벨을 붙인다.
    - **validate** & **select models** & **train models** & **tunning** : 데이터가 마련되면 좋은 성능을 달성할때까지 반복실험 진행 -> 튜닝
    - **publish model** : 배포를 위해 최종모델 선택
- 온라인 프로세스(online process)
  - 오픈후 운영단계에 해당 -> 개발에 가까움 ai 모델을 운영환경에 띄우고, 화면 개발 , 고객사 db 연결 등
  - 이제부터는 운영환경 **스트리밍 데이터(live data)** 처리
- 오버피팅(overfitting)과 일반화(generalization)
  - 일반화(generalization) : 이전에 본적 없는 데이터에 대해서도 잘 수행하는 능력
  - 일반화 능력이 떨어지며 오버피팅된 상태임(새로운 데이터 처리 불가, 기존 답만 제대로 수행)
  - 오버피팅을 회피하고 일반화를 잘하기 위해 , 기존을 데이터를 Training, Validation, Test 로 나누어 사용한다.(8:2:2,9:1:1)
    - Traning set : 학습용 데이터, 정답을 알려주고 학습에 이용
    - Validation set : 정답을 알려주지 않음, 이 데이터를 통해 모델들 튜닝 / 최종 모델 선택
    - Test set : 최종 성능 평가용 데이터
    - Test set 수행이후에 최종 성능향상을 위해 training, validation, test 셋을 기반으로 학습가능
- 학습곡선(learning curve) 확인하기
  - Traning set 의 정답률이 올라가나, validation set 의 정답률은 올라가지 않기 시작하면 오버피팅
- Regularization(정규화) : **오버피팅을 피하고 일반화(generaliztion) 성능향상**이 목적임
  - 데이터 증강 : 데이터를 변조하여 더많이 확보하는 방식
    - 이미지 반전, 크롭, 노이즈, 생상, 명암 , 채도 변화 등과같은 방식
  - Capacity 줄이기
    - Capacity 는 모델의 복잡한 정도를 나타냄
    - 신경망이 여러층이거나, 뉴런수가 많아지면 높아짐
    - Capacity 가 높으면 처리할 데이터의 복잡 다양한 패턴을 잘 처리할수 있음, **하지만 필요이상으로 높으면 그냥 데이터 외움**
  - 조기종료(Early Stopping)
    - 오버피팅이 감지되면 조기종료(validation set 의 개선이 없을때)
  - 드롭아웃(Dropout)
    - 일정비율의 노드(인공뉴런)을 무작위로 끄고 진행.

## 7.인공지능 재활용
- 인공지능 개발의 어려움
  - 구체적이지 않으며 불평확한 타스크
  - 적은 데이터, 낮은 품질의 데이터
  - 다른 도메인 환경(적용환경의 차이에 따른 문제(영상처리시, 조도 각도 등))
- Transfer Learning ( 전이학습 )
  - 기존에 만들어진 딥러닝 모델을 재활용하여 사용하는 기법
  - 이미 만들어 놓은 모델의 아키텍처를 새태스크에 맞게 조금 수정
  - Catastrophic Forgetting (치명적 기억상실 ) : transfer 가 많이되면 기존 데이터를 잊게됨
  - Transfer 를 잘하려면?
    - 레이어 동결
      - 기존데이터를 처리하는 전반부 는 동결, 새타스크를위한 후반부 진행, 일반적으로 딥러닝은 두개 이상의 층으로 구성되며, 초반부에는 구체적이고 기본적인 특징 학습, 후반부에는 특정 태스크를 위한 추상적이고 개념적인 특징 학습
      - 태스크가 많이 비슷하면, 다 동결 후, 후반부 부터 조금씩 동결을 푸는 **Gradual Unfreezing** 도 적용 가능
    - Discriminative fine tuning - 전반부는 조금만 공부, 후반부는 많이 공부(층마다 Learning rate(학습률)의 차별)

## 8.준비된 인공지능
- Pre-Training (사전학습) : Transfer Learning 을 염두에 두고 여러 지식을 미리 학습
  - 이미지 : 이미지넷
  - 영상 : Youtube-8m, 
  - 언어 : 위키피디아, 나무위키, 세종말뭉치 학습
- Self Supervised Learning( 자가지도 학습) - transfer learning 이전에 미리 학습하면 성능 좋아짐
  - 다량의 데이터는 있으나, 라벨이 없는경우, 또는 일부만 있는경우 활용
  - 기계가 라벨을 만들어서 사용
  - Self Supervised Learning 을 활용한 Pre Trained 모델은 방대한 지식 습득을 목적으로 하기에, 모델의 사이즈가 크고, 사전학습 규모가 크다.
  GPU 학습장비나 데이터에 대한 부담이 크나 한번 잘 해 놓으면 어떤과제든 적용가능하다.
  - ex) 일부 라벨링된 데이터를 기반으로 학습뒤, 라벨링 되지 않은 데이터를 추가학습
  - ex) google Bert - 언어라는 분야 전반에 걸친 지식을 학습시켜둠 - 추후 간단한 Transfer learning 으로 좋은 성능 확보

## 9.족집게 학습
- Active Learning(능동학습) : 데이터는 있으나, 라벨이 없고, 라벨을 만들 인력이 있으나, 부족한 경우 사용
- ex : 의료 이미지 인식 -> 의사를 계속 라벨링하라고 할수는 없으니...
- 인공지능 모델이 맞추기 어려운 데이터를 골라서 집중학습
- Active Learning 절차
  - Training a Model : 초기학습 데이터(라벨링된)를 학습
  - Select query : 라벨되지 않은 데이터 중, 어려운 데이터 선별
  - human labeling : 선별된 데이터 라벨링
  - 선별한 라벨 데이터를 기존학습 데이터와 병합한 후 다시 모델 학습
- Query Strategy : 어떻게 잘모르는 데이터를 선별할지?
  - Uncertainty Sampling : 학습된 모델의 판정값을 기반으로 뽑는다. -> ai 가 불확실하다고 판단하는 데이터 추출
    (ex: 강아지, 고양이 확률 50% 면 Uncertain)
  - Query By Commitee : 여러 ai 모델간 결과값이 다른 데이터 선별.
  - Expected Impact : 데이터가 추가될때, 학습된 모델이 가장 많이 변화하는데이터 선별
  - Density Weighted mothod : 밀집된 데이터 선별
  - Core-set approach : 데이터를 최대한 고르게 선별하여 전체 분표를 대표할수 있게함
## 10.중요데이터 추출
많은 데이터중 중요데이터를 선별할수 있어야한다.
- Attention mechanism(어텐션 메커니즘)
  - RNN 의 경우 오래된 데이터는 망각한다. 하지만 어텐션 메커니즘을 통해 과거데이터중 중요부분에 집중하게함
  - 어텐션 스코어(Attention score)
    - 어텐션 스코어는 인공신경망 모델이 각 인코딩 timestep마다 계산된 특징(feature)를 가지고 자동으로 계산하는 0~1사이의 값입니다.
  - 컨텍스트 벡터(Context Vector)
    - 이렇게 어디를 더 살펴보고 어디는 대충 볼지에 대해 어텐션 스코어를 구하고 나면 현재 디코딩할 단어와의 관련성을 반영하여 다시 입력 문장을 인코딩하게 되는데 이는 중요도에 따라 전체 문맥의 정보를 잘 반영하고 있다고 하여 컨텍스트 벡터(Context vector)라고 부릅니다.
- XAI 로서의 어텐션
  - 어텐션 메커니즘은 기계가 판단시 중요하게 생각하는 부분을 우리에게 알려준다. 이를 통해 우리는 결과를 해석할 수 있다. 이를 해석가능한 인공지능(interpretable ai)라고한다.
- Transformer
  - Transformer 인공신경망은 입력 데이터 끼리의 self attention 을 통해 상호 정보 교환을 수행한다. 번역이라고 가정 하면 문장내의 단어들이 서로 정보를 파악하여, 나와 내 주변 단어간의 관계 문맥을 더 잘 파악할 수 있게 되는것
  - 순차적 계산이 필요 없어 RNN 보다 빠르며, CNN 처럼 일부만 보는것이 아니라 전영역을 아우른다.
  - 이해력이 좋은 대신 모델의 크기가 엄청 커지며, 고사양의 하드웨어 스펙을 요구한다.

## 11.스스로 진화하는 인공지능(튜닝)
- 튜닝 대상
  - CNN, RNN, Transfomer 등 어떤 모델을 사용할지
  - 인공신경망 층 수는 몇개로 할지
  - 인공 뉴런의 수(필터 사이즈, 필터 수 등) 은 어떻게 할지
  - 얼만나 큰단위로 학습할지(learning rate)
  - 얼마나 반복학습하지(epoch)
  - 어떤 최적화 기법을 사용할지(optimizer)
  - 손실함수는 어떤것을 쓸지(Cost function)
  - 활성화 함수는 무엇을 사용할지
- AutoML : 자동화된 기계학습
- AutoML 의 역할
  - **Feature Engineering 자동화** : AI 모델을 학습하기 위해 데이터로부터 중요한 특징을 선택하고 인코딩하는 방식 자동화
  - **하이퍼 파라미터 자동탐색** : AI 모델 학습에 필요한 각종 설정값 자동 탐색    
    - 그리드 서치 : 최적화할 하이퍼 파라미터의 값 구간을 일정 단위로 나눈 후, 각 단위 조합을 테스트하여 가장 높은 성능의 조합 선택
      - 단순하지만 하이퍼 파라미터 가 많다면 경우의수가 너무 많아져 오래걸림
    - 랜덤 서치 : 랜던한 조합을 테스트, 그리드에 비교적 빠르게 최적조합 찾음
    - Meta Learner : 최근에는 위의 두가지 방식보다는, 하이퍼파라미터도 모델을 통해 탐색한다. 메타 러너는 RNN 과 강화학습을 통해 하이퍼 파라미터 탐색
    >   - 하이퍼 파라미터 항목 목록
    >     - learning rate : 모델의 파라미터 업데이트를 얼만큼 큰 단위로 할지를 결정하는 학습률
    >     - 미니배치 사이즈 : (mini-batch size)데이터를 얼마나 쪼개어 학습 할지의 단위
    >     - 에폭(epoch) : 데이터를 몇 번 반복 학습할지에 대한 단위
    >     - 이외에도 모멘텀이라든지, 컨볼루션 필터의 수, 스트라이드 등등
  - **아키텍처탐색 자동화** : AI 모델의 구조 자체를 더 효율적인 방향으로
    - NAS(Neural Architecture search) : 인공신경망용 아키텍처 탐색 방법
      -  Meta Learner와 Learner로 이루어져 있어서, Learner가 본 과제를 수행하는 AI 모델이라면 Meta Learner가 어떤 구조의 신경망을 만들면 좋은지, 아키텍쳐 구성을 고민하게 됩니다. Meta Learner는 역시 RNN과 강화학습을 접목한 형식으로 구성해볼 수 있습니다. Meta Learner는 Learner의 인공신경망 아키텍처가 어떻게 구성되면 좋을지를 결정하여, Learner의 태스크 수행 결과를 보상으로 활용합니다.
  - CSP 업체들도 AutoML 제공

## 12.설명가능한 인공지능 (XAI)
딥러닝의 경우 특징 추출부터 판단까지 AI 가 수행하므로 AI 가 판단한 결과가 어떻게 만들어졌는지 추적이 어려움
인공신경망으로 만들어진 AI 는 복잡한 구조로인해 블랙박스로 불려짐

- XAI 수행방법 3가지
  - 기존 AI 모델에 설명할수 있는 모듈을 붙이는방식
    - 어텐션 메커니즘을 활용한 XAI : 딥러닝 모델이 어떤 결정을 내릴때, 입력데이터의 어떤부분에 집중해서 판단했는지를 시각화 
    - 설명방법 학습 ( Learn to explain ) : 딥러닝 모델에 RNN 모듈등을 덧붙여 인간이 이해할수있는 설명을 생성하도록 진행
    - 모듈러 네트워크 : 판정결과가 어떤 모듈경로를 따라 연산되는지 파악, 딥러닝 모델이 해석 가능한 모듈 구송요소로 이루어진 경우
    - Feature Identification : 설명가능한 특징을 학습한 노드를 찾아 그 특징에 설명라벨 붙임
  - 애초에 설명력이 있는 모델을 만드는 방법
    - 의사결정나무, 선형회귀분석은 설명럭이 있는 모델임, 딥러닝은 설명력이 부족한 인공신경망을 사용하므로 딥러닝에서는 미사용
  - 인공신경망처럼 복잡한 블랙박스 모델의 일부분을 설명해 줄 수 있는 다른 모델을 활용하여 유추
    - ex) LIME , SP-LIME
